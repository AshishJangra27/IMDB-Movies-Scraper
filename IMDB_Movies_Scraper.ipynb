{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QG-QUGYbEccd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nPwT1LETFxXb"
      },
      "outputs": [],
      "source": [
        "def download(link, no_of_movies):\n",
        "\n",
        "    file_name = link.lower()\n",
        "    link = 'https://www.imdb.com/search/title/?genres=' + link\n",
        "    res = requests.get(link)\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "    data = []\n",
        "\n",
        "    page = 0\n",
        "\n",
        "    try:\n",
        "        for _ in tqdm(range(int(no_of_movies/50))):\n",
        "\n",
        "            votes = np.nan\n",
        "            gross = np.nan\n",
        "\n",
        "            for m in soup.find('div', class_ = 'lister-list').find_all('div', class_ = 'lister-item mode-advanced'):\n",
        "\n",
        "                try:\n",
        "                    id_         = m.find('div', class_ = 'lister-item-image float-left').find('a').get('href').split('/')[-2].strip()\n",
        "                except:\n",
        "                    id_         = np.nan\n",
        "\n",
        "                try:\n",
        "                    name        = m.find('h3').find('a').text\n",
        "                except:\n",
        "                    name        = np.nan\n",
        "\n",
        "                try:\n",
        "                    year        =  m.find('h3').find('span', class_ = 'lister-item-year').text\n",
        "                except:\n",
        "                    year        = np.nan\n",
        "\n",
        "                try:\n",
        "                    certificate = m.find('p', class_ = \"text-muted\").find('span', class_ = 'certificate').text.strip()\n",
        "                except:\n",
        "                    certificate = np.nan\n",
        "\n",
        "                try:\n",
        "                    duration    = m.find('p', class_ = \"text-muted\").find('span', class_ = 'runtime').text.strip()\n",
        "                except:\n",
        "                    duration    = np.nan\n",
        "\n",
        "                try:    \n",
        "                    genre       = m.find('p', class_ = \"text-muted\").find('span', class_ = 'genre').text.strip()\n",
        "                except:\n",
        "                    genre       = np.nan\n",
        "\n",
        "                try:\n",
        "                    rating      = m.find('div', class_ = 'ratings-bar').find('strong').text.strip()\n",
        "                except:\n",
        "                    rating      = np.nan\n",
        "\n",
        "                try:\n",
        "                    description = m.find_all('p', class_ = 'text-muted')[1].text.strip()\n",
        "                except:\n",
        "                    description = np.nan\n",
        "\n",
        "\n",
        "\n",
        "                try:     # If were having both directors and stars\n",
        "                    temp = BeautifulSoup(str(m.find_all('p')[2]).split('<span class=\"ghost\">|</span>')[0], 'html.parser')\n",
        "                    if ('Director' in temp.text):\n",
        "\n",
        "                        directors_id   = ','.join([i.get('href').split('/')[-2] for i in temp.find_all('a')])\n",
        "                        directors_name = ','.join([i.text.strip() for i in temp.find_all('a')])\n",
        "\n",
        "                    temp = BeautifulSoup(str(m.find_all('p')[2]).split('<span class=\"ghost\">|</span> ')[1], 'html.parser')\n",
        "                    if (\"Star\" in temp.text):\n",
        "\n",
        "                        stars_id   = ','.join([i.get('href').split('/')[-2] for i in temp.find_all('a')])\n",
        "                        stars_name = ','.join([i.text.strip() for i in temp.find_all('a')])\n",
        "                except:\n",
        "\n",
        "                    directors_id   = np.nan\n",
        "                    stars_id       = np.nan\n",
        "                    directors_name = np.nan\n",
        "                    stars_name     = np.nan\n",
        "\n",
        "\n",
        "                try:       # Directors but not Stars\n",
        "                    if('Director' in m.find_all('p')[2].text and 'Stars' not in m.find_all('p')[2].text):\n",
        "                        directors_id   = ','.join([i.get('href') for i in m.find_all('p')[2].find_all('a')])\n",
        "                        directors_name = ','.join([i.text.strip() for i in m.find_all('p')[2].find_all('a')])\n",
        "                except:\n",
        "                    directors_id = np.nan\n",
        "                    directors_name = np.nan\n",
        "\n",
        "                try:        # Stars but not Directors\n",
        "                    if('Stars' in m.find_all('p')[2].text and 'Director' not in m.find_all('p')[2].text):\n",
        "                        stars_id     = ','.join([i.get('href').split('/')[-2] for i in m.find_all('p')[2].find_all('a')])\n",
        "                        stars_name   = ','.join([i.text.strip() for i in m.find_all('p')[2].find_all('a')])\n",
        "                except:\n",
        "                    stars_id = np.nan\n",
        "                    stars_name = np.nan\n",
        "\n",
        "\n",
        "                try:\n",
        "                    if len(m.find('p', class_ = 'sort-num_votes-visible').find_all('span')) == 2:\n",
        "\n",
        "                        if('Vote' in m.find('p', class_ = 'sort-num_votes-visible').find_all('span')[0].text):    \n",
        "                            votes =  m.find('p', class_ = 'sort-num_votes-visible').find_all('span')[1].text.strip()\n",
        "\n",
        "                        elif('Gros' in m.find('p', class_ = 'sort-num_votes-visible').find_all('span')[0].text):    \n",
        "                            gross =  m.find('p', class_ = 'sort-num_votes-visible').find_all('span')[1].text.strip()\n",
        "\n",
        "                    else:\n",
        "                        votes = m.find('p', class_ = 'sort-num_votes-visible').find_all('span')[1].get('data-value')\n",
        "                        gross = m.find('p', class_ = 'sort-num_votes-visible').find_all('span')[4].get('data-value')\n",
        "                except:\n",
        "\n",
        "                    votes = np.nan\n",
        "                    gross = np.nan\n",
        "\n",
        "                data.append([id_, name, year, rating ,certificate, duration, genre, votes , gross ,directors_id, \n",
        "                             directors_name , stars_id, stars_name, description])\n",
        "\n",
        "\n",
        "\n",
        "            next_page_link = 'https://www.imdb.com' + soup.find('a',class_ = 'lister-page-next next-page').get('href')\n",
        "\n",
        "            res = requests.get(next_page_link)\n",
        "            soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    df = pd.DataFrame(data, columns = ['id', 'name', 'year', 'rating' ,'certificate', 'duration', 'genre', 'votes'\n",
        "                               , 'gross_income' ,'directors_id', 'directors_name' , 'stars_id', 'stars_name', \n",
        "                               'description'])\n",
        "    \n",
        "    df.to_csv(file_name + '.csv', index = False)\n",
        "\n",
        "    print(file_name + '.csv is created!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p1  = multiprocessing.Process(target = download , args = ('drama'        , 2580167))\n",
        "p2  = multiprocessing.Process(target = download , args = ('comedy'       , 1936679))\n",
        "p3  = multiprocessing.Process(target = download , args = ('talk-show'    , 1318249))\n",
        "p4  = multiprocessing.Process(target = download , args = ('short'        , 1117705))\n",
        "p5  = multiprocessing.Process(target = download , args = ('romance'      , 938806))\n",
        "p6  = multiprocessing.Process(target = download , args = ('documentary'  , 883693))\n",
        "p7  = multiprocessing.Process(target = download , args = ('news'         , 883562))\n",
        "p8  = multiprocessing.Process(target = download , args = ('family'       , 767551))\n",
        "\n",
        "\n",
        "p1.start()\n",
        "p2.start()\n",
        "p3.start()\n",
        "p4.start()\n",
        "p5.start()\n",
        "p6.start()\n",
        "p7.start()\n",
        "p8.start()\n",
        "\n",
        "p1.join()\n",
        "p2.join()\n",
        "p3.join()\n",
        "p4.join()\n",
        "p5.join()\n",
        "p6.join()\n",
        "p7.join()\n",
        "p8.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lBS49iG3PpV",
        "outputId": "50ba4f84-111c-4483-8b94-d93f246d0efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|‚ñè         | 436/17671 [31:28<20:44:18,  4.33s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "news.csv is created!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|‚ñè         | 719/51603 [34:03<40:10:03,  2.84s/it]\n",
            "  4%|‚ñç         | 602/15351 [34:04<15:31:37,  3.79s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drama.csv is created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|‚ñç         | 1522/38733 [1:12:18<29:27:41,  2.85s/it]\n",
            "  7%|‚ñã         | 1286/17673 [1:12:19<14:53:50,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comedy.csv is created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 8113/18776 [6:49:52<7:48:56,  2.64s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwIiw87v6c-F",
        "outputId": "8983947c-3e3b-46e6-f15d-5ad7cab4e380"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Jul  6 13:22 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jYtUUHPMqKRP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "IMDB Movies Scraper",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}